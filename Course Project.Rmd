---
title: 'Practical Machine Learning Course Project'
author: "AmrB"
date: "January 16, 2015"
output: html_document
---

## Summary

In this project, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har.  Machine learning model will be generated utilizing the activity monitor device data, by using a training set with class labels representing the 6 ways of performing the barbell lifts.  After building the models, the generalization performance will be assessed, and the training model applied to a new set of testing data in order to make predictions.  In a second part of the assignment thr predictions are submitted for grading.

## Input Data

The input data consists of measurments of movements, which include acceleration components of the arms and pitch and roll orientations of the dumbell.

The data used here was downloaded from the course website, where the training and testing data were already partitioned:

[Training Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

[Testing Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The original study is linked below.
[http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)

## Data Analysis and Predictions

Packages caret and knitr are used for this analysis and need to be loaded.  


```{r echo=TRUE, message=FALSE}
  library(caret)
  library(knitr)

  set.seed(140819)
```

We will remove all missing values first.

```{r}
# Read in the training and testing data data
dat.train <- read.csv("pml-training.csv", stringsAsFactors=FALSE)
dat.test <- read.csv("pml-testing.csv", stringsAsFactors=FALSE)

# Function to filter the features
# Remove the features with any missing data
filterData <- function(idf) {
  # Since we have lots of variables, remove any with NA's
  # or have empty strings
  idx.keep <- !sapply(idf, function(x) any(is.na(x)))
  idf <- idf[, idx.keep]
  idx.keep <- !sapply(idf, function(x) any(x==""))
  idf <- idf[, idx.keep]

  # Remove the columns that aren't the predictor variables
  col.rm <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", 
              "cvtd_timestamp", "new_window", "num_window")
  idx.rm <- which(colnames(idf) %in% col.rm)
  idf <- idf[, -idx.rm]
  
  return(idf)
}
  
# Perform the filtering on the datasets
# The training data has classe so we need to
# convert it to a factor for the model building
dat.train <- filterData(dat.train)
dat.train$classe <- factor(dat.train$classe)
  
dat.test <- filterData(dat.test)
```

The filtering process reduced the total number of features to ```r ncol(dat.train)-1```.  The remaining features have complete data, so there is no need for imputation.

After the training and testing sets are prepared, we build prediction models on the training data.  Three models are built using Random Forest, SVM, and KNN.  Parameters will be tuned via 5-fold cross validation.

```{r, echo=TRUE, message=FALSE, cache=TRUE}
  # Create prediction models on the training data
  # Cross validation with trainControl to optimize
  # the model parameters
  # 5-fold cross validation
  cvCtrl <- trainControl(method = "cv", number = 5, allowParallel = TRUE, verboseIter = TRUE)
  # We'll make 3 models that use different approaches and use a voting mechanism for the class predictions
  m1 <- train(classe ~ ., data = dat.train, method = "rf", trControl = cvCtrl)
  m2 <- train(classe ~ ., data = dat.train, method = "svmRadial", trControl = cvCtrl)
  m3 <- train(classe ~ ., data = dat.train, method = "knn", trControl = cvCtrl)
```

With the 3 models built, the cross-validation performance accuracy can be checked.

```{r, echo=TRUE}
  # Make a data frame with the maximum accuracy values from the models obtained
  # via the cross validation on the training data
  acc.tab <- data.frame(Model=c("Random Forest", "SVM (radial)", "KNN"),
                        Accuracy=c(round(max(head(m1$results)$Accuracy), 3),
                                   round(max(head(m2$results)$Accuracy), 3),
                                   round(max(head(m3$results)$Accuracy), 3)))
```

```{r, echo=TRUE, results='asis'}
  kable(acc.tab)
```

The Random Forest model appears to have the highest cross-validation accuracy, with the SVM and KNN slightly lower.

The predictions on the test set data will be done using the 3 models.

```{r, echo=TRUE, message=FALSE}
  # Do the predictions
  test.pred.1 <- predict(m1, dat.test)
  test.pred.2 <- predict(m2, dat.test)
  test.pred.3 <- predict(m3, dat.test)

  # Make a table and check if they all agree
  pred.df <- data.frame(rf.pred = test.pred.1, svm.pred = test.pred.2, knn.pred = test.pred.3)
  pred.df$agree <- with(pred.df, rf.pred == svm.pred && rf.pred == knn.pred)
  all.agree <- all(pred.df$agree)
```

The classifications predictions for the 3 models:
```{r, echo=TRUE, results='asis'}
  colnames(pred.df) <- c("Random Forest", "SVM", "KNN", "All Agree?")
  kable(pred.df)
```

The table shows that the results agree for all models.  Combined with the high accuracy results from the cross-validation procedure, it appears as though we have good prediction models.

The last step of the assignment is to write out the results to test results files to be uploaded for automated grading.  The code below was reused from the course website as suggested for use during the prediction answer submission process.

```{r, echo=TRUE}
  # Looks like they all do; let's write out the prediction files to submit
  # This uses the code supplied by the class instructions
  answers <- pred.df$rf.pred

  pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
      filename = paste0("problem_id_",i,".txt")
      write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
  }
  
  pml_write_files(answers)
```

## Final Predictions

The submitted predictions were all evaluated as correct.




